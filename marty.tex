\documentclass[a4paper,12pt,twoside,BCOR=10mm]{scrbook}

% Packages
\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage[icelandic, english]{babel}
\usepackage{t1enc}
\usepackage{graphicx}
\usepackage[intoc]{nomencl}
\usepackage{enumerate,color}
\usepackage{url}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage{appendix}
\usepackage{eso-pic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[nottoc]{tocbibind}
\usepackage[sort&compress,authoryear]{natbib}
\usepackage[sf,normalsize]{subfigure}
\usepackage[format=plain,labelformat=simple,labelsep=colon]{caption}
\usepackage{placeins}
\usepackage{tabularx}
% Configurations
\graphicspath{{img/}{../img/}}

\setlength{\parskip}{\baselineskip}
\setlength{\parindent}{0cm}
\raggedbottom
% \setkomafont{subsection}{\normalfont\sffamily}

% Eins og templatið á að vera
% \setkomafont{captionlabel}{\itshape}
% \setkomafont{caption}{\itshape}

% Mun fallegri lausn
\setkomafont{captionlabel}{\itshape}
\setkomafont{caption}{\itshape}
\setkomafont{section}{\FloatBarrier\Large}
\setcapwidth[l]{\textwidth}
\setcapindent{1em}


% Times new roman
%\usepackage[T1]{fontenc}
%\usepackage{mathptmx}

%%%%%%%%%%% MODIFY THESE LINES ONLY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\thesisyear{2013}       						% Year thesis submitted
\def\thesismonth{October}						% Month thesis submitted
\def\thesisauthor{Baldur Þór Emilsson}					% Thesis authoreiningaraðferðinni
\def\thesistitle{Marty: Application Development and Testing with Production Data in PostgreSQL} % Title of thesis
\def\thesisshorttitle{Development and Testing with Production Data} 	% Title of thesis
\def\thesiscredits{60} 							% Credits awarded for the project
\def\thesissubject{Computer Science}
\def\thesiskind{M.Sc.}							% Masters of PhD thesis
\def\thesiskindformal{Magister Scientiarum}				% Masters of PhD thesis
\def\thesisnroftutors{1}						% Number of tutors
\def\thesisschool{School of Engineering and {Natural Sciences}}		% School
\def\thesisfaculty{Industrial Engineering,\\Mechanical Engineering and\\Computer Science} % Faculty
\def\thesisaddress{Hjarðarhaga 2-6}					% Office address
\def\thesispostalcode{107, Reykjavik}					% Office address
\def\thesistelephone{525 4700}						% Office telephone
\def\thesistutors{Hjálmtýr Hafsteinsson}
\def\thesisrepresentative{XXNN3}					% Tutors name
\def\thesisPrinting{Háskólaprent, Fálkagata 2, 107 Reykjavík}

% Function to add footer to frontpage
\newcommand\BackgroundPic{
\put(0,0){
\parbox[b][\paperheight]{\paperwidth}{
\vfill
\centering
\hspace*{-0.6cm}
\includegraphics[width=\paperwidth,height=\paperheight,
keepaspectratio]{foot}
}}
\setlength{\unitlength}{\paperwidth}
\begin{picture}(0,0)(0,-0.15)
\put(0,0){\color{white}\parbox{1\paperwidth}{\centering\bfseries\sffamily \Large Faculty of \thesisfaculty \\
University of Iceland\\
\thesisyear}}
\end{picture}
}

\begin{document}

\begin{titlepage}
\thispagestyle{empty}
\AddToShipoutPicture*{\BackgroundPic}
%
\begin{center}
\vspace*{1cm}
\includegraphics[width=43.6mm]{ui_1_cmyk}\\
\vspace*{3.0cm}
\huge \sffamily \bfseries \thesistitle

\vspace*{5.5cm}
\normalfont \Large \sffamily \thesisauthor
\AddToShipoutPicture*{\BackgroundPic}
\vfill

\end{center}

\newpage 
\thispagestyle{empty} \mbox{}
\newpage
\vspace*{1.35cm}
\thispagestyle{empty}
\begin{center}

\Large \textbf{\sffamily{\MakeUppercase{\thesistitle}}} \\

\vspace*{1.7cm}
\sffamily{\thesisauthor} \\
\vspace*{1.8cm}
\normalsize \thesiscredits~ECTS thesis submitted in partial fulfillment of a \\
\textit{\thesiskindformal} degree in \thesissubject

\vspace*{1.0cm}
\large
\ifnum\thesisnroftutors >1 Advisors \\ \thesistutors \\ \vspace*{0.4cm}
\else Advisor \\ \thesistutors \\ \vspace*{1.04cm}
\fi
Faculty Representative \\
\thesisrepresentative

\vfill
Faculty of \thesisfaculty \\
\thesisschool \\
University of Iceland \\
Reykjavik, \thesismonth~\thesisyear
\newpage
\end{center}
 \newpage
 \thispagestyle{empty}
 \mbox{} \vfill
 % \setcounter{page}{0} \renewcommand{\baselinestretch}{1.5}\normalsize
 \sffamily{\thesistitle} \\
 \sffamily{\thesisshorttitle} \\
 \thesiscredits ~ECTS thesis submitted in partial fulfillment of a \thesiskind~degree in \thesissubject
\\ \\
Copyright \textcopyright~\thesisyear~ \thesisauthor \\
All rights reserved \\


Faculty of \thesisfaculty \\
\thesisschool \\
University of Iceland \\
\thesisaddress \\
\thesispostalcode, Reykjavik \\
Iceland

Telephone: \thesistelephone \\ \\
\vspace*{\lineskip}

Bibliographic information: \\
\thesisauthor, \thesisyear, \thesistitle, \thesiskind~thesis, Faculty of \thesisfaculty, University of Iceland. \\

Printing: \thesisPrinting \\
Reykjavik, Iceland, \thesismonth~\thesisyear \\
\newpage
\end{titlepage}


\pagenumbering{roman}

\setcounter{page}{3}
\section*{\huge Abstract}
Marty is a proof-of-concept prototype for a framework that offers convenient application development and testing against data used in production that is stored in the PostgreSQL database management system. It is designed for minimal overhead and configuration on production servers while offering quick and simple database initialization on development and testing servers. This opens the possibility for application testing on production data with minimal effort, which complements conventional testing datasets and helps preventing bugs from entering production code which were not caught with the conventional datasets.
\vfill \vspace*{1cm}
\section*{\huge Útdráttur}
Marty er hugbúnaðarlausn sem býður upp á þægilegt þróunar- og prófunarumhverfi fyrir forrit sem nota PostgreSQL gagnagrunnskerfið. Hún er hönnuð til að nota gögn úr gagnagrunnum sem keyra í raunumhverfi án þess að hafa neikvæð áhrif á afköst netþjónanna sem grunnarnir keyra á og án mikilla breytinga á uppsetningu þeirra en bjóða á sama tíma upp á fljótlega og einfalda uppsetningu þróunar- og prófunargagnagrunna. Það opnar fyrir möguleikann á hugbúnaðaprófunum með raungögnum án mikillar fyrirhafnar sem geta keyrt samhliða prófunum með hefðbundin prófunargagnasett og hjálpað við að uppræta villur sem koma ekki í ljós með hefðbundnum prófunum.
\vfill
\newpage

\tableofcontents
\listoffigures
\listoftables

\chapter*{Abbreviations}
\addcontentsline{toc}{chapter}{Abbreviations}
Í þessum kafla mega koma fram listar yfir skammstafanir og/eða breytuheiti. Gefið kaflanum nafn við hæfi, t.d. Skammstafanir eða Breytuheiti. Þessum kafla má sleppa ef hans er ekki þörf. \\

The section could be titled: Glossary, Variable Names, etc.

\chapter*{Acknowledgments}
\addcontentsline{toc}{chapter}{Acknowledgments}
Í þessum kafla koma fram þakkir til þeirra sem hafa styrkt rannsóknina með fjárframlögum, aðstöðu eða vinnu. T.d. styrktarsjóðir, fyrirtæki, leiðbeinendur, og aðrir aðilar sem hafa á einhvern hátt aðstoðað við gerð verkefnisins, þ.m.t. vinir og fjölskylda ef við á. Þakkir byrja á oddatölusíðu (hægri síðu).


\chapter{Introduction}
\pagenumbering{arabic}
\setcounter{page}{1}
Database management systems (DBMS) are used as datastores in many different systems in various fields. They are rarely used as standalone products and are usually used to store data from other applications. These applications are often in constant development with short development cycles, which include both manual and automated testing. Those  tests are often run against datasets that are created to test for specific conditions and ideally they help with catching all bugs before they enter production. However, many projects can benefit from tests that are run against data from the production environment, either to complement the testing datasets or to provide data to test against in situatios where no testing datasets exist. The main disadvantage of using production data in testing is that cloning a large database can take a long time which slows down testing and development and it adds an overhead to the database in production which can have negative effects on the performance of the application in the production environment.

The goal of Marty is to offer a convenient and relatively efficient way to run tests for applications that use the PostgreSQL (Postgres) DBMS against the live data on the production servers without adding overhead to them. This is achieved by creating a testing database with empty tables that are populated  when they are first queried. This saves time as only the tables which are used in the tests are populated and no time is spent copying the data for the other tables, which remain empty. The data is not copied directly from the production server but from another instance of Postgres that stores a copy of the production data. This is done to ensure the consistency of the data in the cloned databases and also to minimize the load on the production database. A byproduct of this architecture is the possibility to inspect the state of the production database as it was at a certain point in time and also to inspect the changes that were made to the database, both to the schema and to the data, in the order that they were executed.

% TODO rewrite this section when the thesis is complete: The rest of this thesis is organized as follows: chapter 2 contains details about the history of Postgres and its internals that are relevant to Marty. In chapter 3 the design of Marty is described along with reasoning for why this design was chosen and some details about the implementation of Marty. The fourth and final chapter contains conclusions and future work.

\chapter{Goals and purpose of Marty}
The goal of the development of Marty was to create an application that enabled its users to clone a running database quickly. The original idea was that software developers and testers should be able to clone a database that is used in production and stores large amounts of data that would normally take a considerable time to copy to another server. Marty should speed up development and testing by reducing the time it takes to clone the production database and should also use techniques that reduce or prevent any negative impact that the cloning would have on the performance of the production database. 

Although the initial idea was for production databases to be cloned there should not be anything that prevents Marty to be used for other kinds of databases, such as databases that are dedicated to storing test datasets that never enter production, as long as those databases fulfill the requirements for Marty.

The emphasis in the design and development of Marty was to minimize the time from when the cloning of a database was initialized and until the newly created clone could be used for testing. The performance of the newly created database was not a high priority as it was not intended to be used in a performance critical environment. Thus Marty is not a solution that should be used to create clones of a database that are to be used for load balancing or failover or serve any other role in the production envorinment of an application.

Marty is supposed to be used in an environment where a one or a few databases need to be cloned regularly. The architecture that was chosen for Marty requires a system administrator to set up and configure Marty for the environment where it is used. This involves running a dedicated Postgres instance that is used as a reference when the clones are created and also configuring the production server to work with this dedicated instance in a certain way, which might require the production server to be restarted. It should therefore be clear that Marty is not suited for cloning a database that only needs to be cloned for a limited number of times and should be most useful when the database to be cloned is large enough, or is expected to grow enough, that the time saved by using Marty justifies the initial setup.


\chapter{Architecture}
This chapter contains a detailed description of the architecture and design of Marty with references to the internal parts of Postgres that are releveant for each part of Marty. It also includes ideas that were not pursued and the reasoning behind the decisions that were made in the design process.

\section{Overview}
Marty consists of a few parts that serve different pruposes, see figure \ref{architecture-overview}. Developers and testers that use Marty in their work create database \textit{clones}. These databases are clones of the \textit{master} database, which can be a database that is used in a production environment. When the clones are created they are initialized with a copy of all the tables in the master, but the tables remain empty until they are queried by a user or an application. The design of the clone databases is discuessed in chapter \ref{sec:clones}.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{architecture}
  \caption{An overview of the architecture of Marty}
  \label{architecture-overview}
\end{figure}

Marty does not inspect the schema of the master database directly when it creates the tabes in the clone databases. Instead it queries another database that is called \textit{history}. This database contains a copy of the schema of the master database along with a copy of the data that the master contains. When the clones need to populate their tables they also use this history database as a reference. The reason for using another database to store a copy of the schema and data of the master database is discussed in chapter TODO, along with a description of the design of the history database.

As the name suggests the history database contains not only a copy of the current version of the master database but also a copy of previous versions. To update the history database with new versions of the master and to keep it in sync with the changes that are made on the master Marty uses a log from the master that is called the \textit{write-ahead log} or \textit{WAL}. It does not read this log directly but uses a specially patched instance of Postgres to read the contents of the log. This instance is called \textit{slave} and it outputs information that Marty can use to update the history database with all the changes that have been made in the master database. The reason for keeping old versions of the master database and the relationship between the master, slave and history databases is described in detail in chapter TODO.

\section{The clones}
\label{sec:clones}
A clone database is a standard Postgres database. It uses two Postgres extensions; the \textit{PL/pgSQL} extension that enables users to create stored procedures, and the \textit{dblink} extension which enables users to query another database directly from the clone database without using any external scripts or programs. The clone can run on a local instance of Postgres on the developers' or testers' computer as long as the history database is accessible from that computer. More than one clone database can run in parallel on the same instance of Postgres so each user can use many clones at the same time.

To create a clone the user creates a new, empty database. She then initializes it with Marty. After the clone has been initialized it contains all the schemas that are found in the master database and a copy of all the tables from each schema. The tables remain empty until they are first queried to save time in the initialization. To populate the tables when they are queried Marty actually creates views insetad of tables and keeps the actual tables elsewhere, see figure \ref{clone-architecture}. The views look like the tables that are in the master database and in addition to the schemas from the master Marty creates a schema in the clone that is called \textit{marty}. Therein lie the actual tables that store the data from the master which are empty when the clone has been initialized.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.5\textwidth]{clone-architecture}
  \caption{Table layout in a clone database}
  \medskip
  \footnotesize
  An example of where a clone database stores the data from the master. In the clone the table \textit{mytable} is actually a view that returns results from the table \textit{data\_myschema\_mytable\_1} which is in the \textit{marty} schema. The name of the table and its columns as well as the purpose of the extra columns is described in chapter TODO
  \label{clone-architecture}
\end{figure}

When the user queries the views Postgres executes a PL/pgSQL function called \textit{view\_select} that Marty created. This function returns the results from the data tables in the marty schema after it populates them with data from the history database. Marty keeps track of which tables have been populated and which ones are still empty in a table called \textit{bookkeeping} which is in the marty schema. This table also contains information that Marty uses to look up the correct data in the history database. A detailed description of the bookkeeping table and the views, the \textit{view\_select} function and the data tables can be found in chapter TODO.

\subsection{-}

\section{The history database}
-

\section{Advantages and Drawbacks}
The current architecture of Marty that is described in this chapter was chosen because of its simplicity and because it could be implemented in high level code (PL/pgSQL instead of C) which sped up prototyping and simplified the deployment of Marty. However, it has a few drawbacks which make it unsuitable for a production ready version of Marty. The main drawback is the lack of optimization for queries from the clones to the history database; when a user queries a table in a clone database it fetches the complete contents of that table from the history database even if the query should only return a small part of it to the user. Another issue is the creation of indexes for the tables in the clones; the user can not create indexes for the tables in the clone like she would create them on the master database. This is because the tables that the user expects to find in the clone database are actually views and it is not possible to create indexes for views in Postgres.

See chapter TODO for a discussion of the current status of Marty, the limitations of the current version and ideas for future works and improvements.


\chapter{Postgres}
This chapter provides a short history of PostgreSQL along with an overview of the features and internal design that Marty builds on.

\section{History}
Postgres is an SQL based DBMS that originated at the University of California, Berkely in the 1980's. It was based on another DBMS, Ingres, and was released as a free and open source software in 1995. It is developed by a global community under the name PostgreSQL Global Development Group, with a core team consisting of a half a dozen members and a large number of other contributors. It is written in C and runs on multiple platforms.

Postgres is very mature and has a large number of features, including conformance with a large part of the SQL standard and a support for extension modules. Many modules have been created to add new data types, offer new scripting languages for stored procedures and to add functionality for specific types of data, such as geographical information. It has a very extensive documentation and an active community that offers support for users through mailing lists and IRC channels.

Many companies offer commercial support and products based on Postgres with many more using it as a part of their internal systems. It is used by government organizations and universities and many free and open source software projects.

\section{Data storage}
Postgres stores all data in a \textit{cluster}. The cluster is a directory in the file system and contains many different files and subdirectories. Each cluster can have many databases for logical separation of data. The data is stored in relations in the database which can be of a few different kinds, see table 1. Each database has a subdirectory in the filesystem and each relation is stored in a separate file inside that subdirectory.

% TODO add table 1 and change reference to it

The relation files contain \textit{pages}, which are usually 8 kB, and the data is stored inside those pages in \textit{tuples}. Regular tables store each row in a spearate tuple.

% TODO mynd sem lýsir hverni töflur eru geymdar í skrá?

The tables have a few \textit{system columns} that Postgres uses for internal references, see table 2. One of those columns is \textit{ctid} which is a virtual column whose value is not actually stored in disk but is derived from the location of the page and tuple where the row is stored. The page number is zero-based while the tuple number is one-based, so the first row in a table has the ctid value if \textit{(0,1)}. This value is used by Marty to reference individual rows.

% TODO add table 2 and change reference

\section{Transactions}
Postgres supports multiple concurrent connections to the same database, which can read and write to the same relations at the same time. To avoid inconsistency, such as one user reading partially written data by another user, Postgres uses the well known technique of wrapping queries that alter the state of the cluster in a transaction. Each transaction acquires locks for its operations, e.g. read and write locks for relations, before it can be executed. To boost performance Postgres uses a technique called multiversion concurrency control (MVCC) which enables different transactions to read and write to the same relation at the same time without causing inconsistency.

Each transaction has a unique ID (xid) and when a row is inserted into a table the current xid is recorded in the \textit{xmin} system field, see table 2. When the row is deleted its contents are not immediately removed from the database but the current xid is stored in another system field, \textit{xmax}. An update is a combination of those two operations, the row with the old value is marked as deleted and another row is inserted with the new value. When the table is then queried Postgres uses the current xid (if the query is part of a transaction), or the latest xid, as a reference to filter which rows are visible. Only rows whose \textit{xmin} is equal to or less than the reference xid and whose \textit{xmax} is greater (or zero, for rows that have not been deleted) are returned.

% TODO fix reference to table 2
% TODO Mynd: útskýring á transactions og xmin og xmax

The xids are 32 bit integers that wrap and are reused. Before Postgres reuses old xids it runs an operation called \textit{vacuum} where old xids are replaced with a value that is always lower than the current xid. If the xmax field contains an xid that can be replaced with this value the row can no longer be read by any tranaction and is thus removed permanently.

% TODO explain vacuum better?

\section{Write Ahead Log}
An important concern of Postgres is the correctness and consistency of data inserted into the system. No hardware can offer 100\% reliability and Postgres is designed with that in mind. Every operation that in some way alters the state of the database in Postgres is logged in a so called \textit{write ahead log}, or WAL. This is a common feature of many database systems and its name is descriptive of the fact that all changes must first be written to this log before the operation can be considered complete. That means that a transaction must both make changes to the relation files in the cluster and log those changes to the WAL before finishing successfully.

The reason for this redundancy, which appears to add unnecessary overhead to all transactions, becomes clear when the underlying storage hardware is investigated. All storage hardware, whether it is a rotating magnetic disk or a solid state flash memory, has a mich higher performance when reading or writing sequential data than when the data is scattered throughout the storage. This performance difference is usually many orders of magnitudes and a program that runs in a reasonable time when working with sequential data might become unusable if the data was scattered. This has a direct effect on the design of Postgres beause transactions do often make changes to many different relations in a database, thos relations are stored in different files in different places on the hard drive and writing to all those files can take considerable time. To prevent transactions from stalling while this data is written to the disk the updated data is instead cached in the memory and is flushed to the disk at a later time. 
This dramatically improves the performance of Postgres and enables it to handle a very high number of queries that are necessary or many large applications. However it also introduces the risk of loss of data that was supposed to be safely stored in the database. In case of a system crash, because of a power outage or a hardware failure, the relation data that has not been flushed to the disk whould be lost if no actions were taken to ensure data safety. The strategy that was chosen in the Postgres design is therefore to keep a log of all changes that are made to the databases. This log is always flushed to the disk before a transaction is finished and in the case of data loss due to system failure the changes in the log are replayed into the database The performance hit of writing the WAL to the disk is less than writing the relation data to the disk by many orders of magnitude. This is due to the append-only natre of the log which means that it is written sequentially to the disk and makes it an acceptable 
solution to ensure data safety while keeping performance high.

% TODO IS THIS REALLY NECESSARY? :s
% TODO add reference and picture: http://queue.acm.org/detail.cfm?id=1563874

Each time that Postgres is started it enters recovery mode where it reads the WAL and applies all changes that were not successfully written to the cluster. This operation is realtively fast as the WAL contains the same raw binary data that is in the cluster files and Postgres can copy that data directy into them. This means that individual SQL queries can not be looked up directly in the WAL and to be able to understand its contents one must have access to the database cluster that it is used to log.

The WAL is not stored in one big file but is broken into smaller files, usually 16MB each. This simplifies many operations that Postgres executes when working with the WAL, such as recycling and archiving. When all contents of a WAL file have been written to the relation files in the cluster and those files have been flushed to the disk the WAL file is no longer necessary. Postgres either deletes that file or recycles it; the filename is changed to represent the latest WAL file and new WAL records from the latest transaction are written into that file, overwriting the old contents of the file.

Postgres can be configured to automatically archive the WAL files when it stops writing to them and switches to the next file. Before Postgres is started the database administrator can configure it to execute a shell command to copy the WAL file to be archived. The command can be of any length and can run any programs that are installed on the server, so the archiving strategy can be tailored to fit the needs of each project. It could archive the files to another hard drive on the same server, to another server on the same network or into the cloud.

The WAL files are not of much use by themselves. In the case of a catastrophic failure where the data in the Postgres server is corrupted or lost it is not enough to have a copy of the WAL files to restore the database. For that to be possible it is necessary to keep a backup of the cluster files. Another instance of Postgres can then use that backup to run the databases as they were at the time that the backup was taken. The archived WAL files can then be used to update the databases in this server and the only data that is lost is the data in the WAL files that had not been archived yet.

% TODO Please simplify, brutally

\section{Standby servers}
When the data in a Postgres server becomes unavailable as a result of a crash or corruption it is possible to use a backup of the cluster files and archived WAL files to restore the data with minimal data loss. Configuring and starting this backup takes time which results in a downtime of the system or application that uses the databases in Postgres. To minimize this downtime it is possible to start the backup server \textit{before} the primary server fails. This is called a \textit{standby server} and it runs alongside the primary waiting for new WAL files to arrive at the archive location. Those WAL iles are immediately replayed into the server so it is always up to date. When a failure occurs in the Primary server the standby server can immediately be configured to take over and run as the primary server.

The standby server is not only a backup solution. It can be queried with read-only requests, which can help with distributing the load from the primary server and on to backup servers where it is acceptible that the returned data might not be up to date. This feature is used by Marty when it reads the contents of the WAL files, this is described in detail in chapter X.

% TODO update reference to chapter X.
% TODO Shorten and simplify, explain more in terms of Marty than in terms of Postgres

\chapter{Marty}
% TODO Þessi kafli inniheldur...

\section{Design considerations}
The idea behind Marty is that developers should be able to clone a database quickly, ideally in a few seconds. This is simple for small databases that are up to a few megabytes but cloning a larger database that contains hunderds of megabytes or many gigabytes of data can take minutes instead of seconds. This makes cloning a large database unfeasible in many situations where it might be beneficial for the development of applications that use Postgres.

Instead of cloning the whole database before it is used it would be possible to clone a part of it and then fetch the missing data when it is needed. This is what Marty does; it populates a database with empty tables and fetches their contents when they are queried. This reduces cloning time considerably with the drawback of longer query times for the initial queries to the tables in the cloned database, when the tables are queried for the first time they are empty and need to be populated with the data from the master database.

In the time between setting up the empty tables and populating them with data the master database can change. This can lead to missing and inconsistent data in the cloned database, e.g. if a table is dropped on the master its data is not available for the clone. If the table is dropped on the master after it is created on the clone but before it is populated it will stay empty on the clone which might break foreign key constraints in other tables. Therefore Marty needs to be able to fetch the data as it was in the master at the time of cloning. To be able to do that it is necessary to keep a log of the changes and the different states of the master database so that the cloned databses can fetch the data as it was at the time of cloning. To be able to do that it is necessary to keep a log of the changes and the different states of the master database so that the cloned databases can fetch the data as it was at a certain point in time. This is not a standard feature of Postgres so to be able to use this 
technique of initializing empty tables and populating them later the changes to the master database must be logged in some way to make past states o the database available.

There are a few ways to create such a log, following is a list of a few method that were considered and their pros and cons, ending wht the one that was chosen for Marty.

\subsection{Keep a log in the master database}
Instead of deleting rows from the tables in the master database they would just be marked as deleted. Similarly when a row is updated the new values would be written to a new row and the old row would be marked as deleted. This is similar to the way that Postres uses MVCC internally.

This method could keep the setup of the cloned databases simple as they would query the master database in the same way as other applications, only with a different \textit{where} clause to limit the results to rows that were created before a specific point in time. However it would complicate the table schema of the master database as well as all queries to it. Old databases would need to be refactored and the cloned databases would add an overhead to the master database when the contents of the tables are fetched, which might degrade the performance of the master server. This would lead to more mistakes being made in database administration and development of applications that use Postgres and would make Marty unappealing or unsuitable for many environments.

\subsection{Use triggers to log changes}
All changes would be logged to another database by using triggers on the master. This setup can reduce the performance impact that the first solution would bring as the cloned databases would query the other database that keeps the log instead of the master. It would complicate the database administration because a separate database is created for the log and a trigger would need to be created for every table in the master database. This might lead to errors both in the master, which might have a negative effect on performance and stability, and errors in the database that keeps the log and the cloned databases, as missing or wrong triggers would result in inconsistent or missing data. The cost of creating the triggers and the potential for errors and performance issues on the master might hinder many administrators from trying Marty, as the risk would outweigh the potential benefits. For these reasons this approach was not taken.

\subsection{Use the MVCC to make old data available}
Postgres does aldready store old versions of the data in the database. As described in chapter X Postgres implements MVCC so deleted rows are not removed but only marked as deleted and when rows are updated they are marked as deleted and the new values are recorded in a new row. This is only for internal use and database users do not get any access to or control over the data that Postgres has marked as deleted.

To be able to use this old data it would be necessary to patch Postgres. A patch like that would create an interface for the database users to access the old versions of the data, most likely by adding an SQL command. This approach could simplify database administration as the same database is used by the cloned databases and other applications. However this approach has similar problems as the previous two, namely the cost of setting it up - it would require the master database to be moved to a patched instance of Postgres - as well as security concerns and possible performance as the previous two, namely the cost of setting it up - it would require the master database to be moved to a patched instance of Postgres - as well as security concerns and possible performance issues. A new code brings potential security risks, especially when added to a complex system such as Postgres, and the cloned databases would add performance overhead to the master which might already be in heavy use. Therefore this method 
was ruled out.

% TODO fix reference to chapter X

\subsection{Read the changes from the WAL}
All changes to the master server are already logged to the WAL. Postgres can be configured to archive the WAL files automatically to a server where they can be inspected without additional performance overhead on the master server. The only change that is needed on the master server s the configuration for the WAL archiving, if it has not already been configured. Reading the contents of the WAL is more problematic as there is no method for inspecting its contents. Postgres uses internal functions to read and apply the WAL to th cluster that are not accessible for plugins or third party applications so reading the WAL requires an application that reimplements these functions or a modified version of Postgres that logs the changes in a way that is usable for Marty. This application or modified Postgres instance would add administrative overhead but would run separately from the master Postgres instance, possibly on another server, and would in no way interfere with the master server. This could make Marty more 
appealing for database administrators that are concerned with stability and correctness.

This method was choen because it did not share the stability and security problems that the other methods introducedto the master server. An important concern in the development of Marty was the ability to set it up and use it without making changes to or requiring refactoring of the master database and this method of accessing the changes and old data of the master database is in ine with that goal.

\section{Architecture}
Marty is designed to create clones of one \textit{master} database. This master could be used in production to store live data or it could be used to store mock data to be used for development and testing. Marty is configured to create \textit{clones} of this database and there can be multiple concurrent clones in use at the same time. These clones represent the master database as it was when the clone was created, if it is necessary to get an updated version that contains more recent data another clone must be created.

When the user creates a clone she runs a script that reads the database schema of the master and creates the tables and other database objects in the clone. The schema is not read directly rom the master but from another database called \textit{history}. The purpose of this database is to keep a record of the contents and shema of the master as it was at any given point in time. It serves two functions; to reduce the load on the master when the clones are created and when they fetch the contents of the tables and also to keep a log of all the changes in the master so that the clones can query the contents of the tables as it was at the time of the creation of the clone. As described in chapter X this is necessary because of the way that the cones are created, empty tables that are populated when they are first queried.

% TODO fix reference to chapter X
% TODO add figure

To create the history database Marty starts by reading the schema of the master and its data and record it in the history database. This creates the first version of the data in the history database. Subsequent versions are created when Marty reads the WAL from the master. As previously described it is not possible to read the contents of the WAL directly, it is necessary to use the cluster files as a reference. Thus before Marty is started the cluster files of the master are copied and another instance of Postgres is started that uses the copy of the cluster. This instance is updated with the data in the WAL from the master and is called \textit{slave}, like it is referenced in the Postgres documentation. Marty initializes the history by inspecting the contents of the slave and continues to update the history with the data that the slave reads from the WAL from the master. This setup is illustrated in figure X.

% TODO add reference to Postgres doc: http://www.postgresql.org/docs/9.3/static/high-availability.html
% TODO fix reference to figure X

The goal for the current version of Marty was a working proof-of-concept prototype. It lacks many features that are critical for its use as a part of the development and testing of applications and it only supports the most basic functions of a database; selecting, inserting, updateing and deleting data. The features that are missing are e.g. table constraints (primary and foreign keys), indexes, sequences and more. I was not concerned with copying the roles (users) or their privileges from the master over to the clones so the user(s) on the clones have all availabel privileges for all objects in the database.

\section{History database}
The history database keeps records about exactly one master database. It keeps records on both the data definitions (schema) of master and the data stored in its relations.

The current version of Marty records information about all schemas in master (that is Postgres schemas that are used for grouping tables) but is only concerned with ordinary table relations, it skips indexes, sequences and all other types of relations that are not ordinary tables. The information that is recorded about the tables is just the minimum that is necessary to reconstruct the contents and schema of tables that use simple standard column types, so no constraints (primary / foreign keys or unique values) or default values nad not arrays or composite types.

All data in history is versioned. When data is inserted the time of insertion is recorded and when data is deleted it is only marked as unavailable after that point in time. This is similar to the way Postgres uses MVCC internally and Marty keeps track of the data versions in a table called \textit{marty\_updates} (see table X).

% TODO insert table and fix reference

\subsection{Data definition}
Marty records information about the schemas in master to the table \textit{marty\_schemas} (see table X) and information about the tables in master and their columns is recorded to \textit{marty\_tables} (see table X) and \textit{marty\_columns} (see table X) respectively.

% TODO insert tables and fix references

The information in those tables is read from the system catalogs in Postgres. The schema information comes from \textit{pg\_namespace}, the table information is from \textit{pg\_class} and the column information is from \textit{pg\_attributes} except for the type names which are stored in \textit{pg\_type}.

\subsection{Content}
For each user created table in master there is a corresponding data table created in history that contains its contents. When table information has been added to \textit{marty\_tables} and \textit{marty\_columns} this table is created. Its name is on the form \textit{data\_[schema]\_[table]\_[update]} where \textit{schema} is the name of the schema it belongs to, \textit{table} is the name of the table in master and \textit{update} is the ID of the version this table was created in. This naming convention allows tables from all schemas in master to be created in the default schema in history without any naming conflicts, which simplifies the management of the content tables. It also prevents conflicts in the case of table name reuse (e.g. a new table uses the name of an old table that has been dropped) by including the version ID.

The tables contain the same columns as the original tables in master with the addition of three metadata columns, see table X. The first one, \textit{data\_ctid}, is used as an ID to locate rows when they are deleted or updated. It contains the value of the \textit{ctid} column in the original master table. The other two, \textit{start} and \textit{stop} are foreign keys to \textit{marty\_update} and are used to decide which rows are part of which version of the data.

The other columns contain the same data as the original table in master but their names are on the form \textit{data\_[column]\_[update]} where \textit{column} is the name of the column in master and \textit{update} is the version ID, similar to the \textit{update} part in the table names. This naming convention is used to prevent conflicts both in the case of column name reuse (when a new column uses the name of an old, dropped column in the same table) and with the names o the metadata columns.

% TODO insert table

\end{document}
